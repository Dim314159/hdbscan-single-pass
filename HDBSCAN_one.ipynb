{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Qyu03gMd5qt4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from collections import defaultdict\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "XFYZGtm65qt5"
      },
      "outputs": [],
      "source": [
        "# cpu_gpu_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# #cpu_gpu_device = torch.device('cpu')\n",
        "# print(f\"Using device: {cpu_gpu_device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKT0vyvO5qt6"
      },
      "source": [
        "# * MyHDBScan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_W-OuKB5qt7"
      },
      "source": [
        "## ** MST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "sJyp25065qt7"
      },
      "outputs": [],
      "source": [
        "class MST:\n",
        "    def __init__(\n",
        "            self,\n",
        "            number_of_nearest_neighbors=5,\n",
        "            cpu_gpu_device='cpu',\n",
        "            ):\n",
        "        # number of nearest neighbors\n",
        "        self.k = number_of_nearest_neighbors\n",
        "        self.cpu_gpu_device = cpu_gpu_device\n",
        "\n",
        "    def mutual_reachability_distance(self, X):\n",
        "        \"\"\"\n",
        "        Compute the mutual reachability distance matrix using PyTorch and CUDA.\n",
        "        \"\"\"\n",
        "        X = torch.tensor(X, dtype=torch.float32, device=self.cpu_gpu_device)  # Move data to device\n",
        "        pairwise_dist = torch.cdist(X, X)  # Compute pairwise Euclidean distances\n",
        "        knn_distances, _ = torch.topk(pairwise_dist, self.k + 1, largest=False)  # k-NN distances\n",
        "        core_distances = knn_distances[:, -1].unsqueeze(1)  # Core distance is k-th neighbor\n",
        "\n",
        "        # Compute mutual reachability distance\n",
        "        mutual_dist_matrix = torch.max(pairwise_dist, core_distances)\n",
        "        self.mutual_dist_matrix = torch.max(mutual_dist_matrix, core_distances.T)\n",
        "\n",
        "    def minimum_spanning_tree(self):\n",
        "        \"\"\"\n",
        "        Compute the Minimum Spanning Tree using Kruskalâ€™s algorithm in PyTorch.\n",
        "        \"\"\"\n",
        "        n = self.mutual_dist_matrix.shape[0]\n",
        "        triu_indices = torch.triu_indices(n, n, 1, device=self.cpu_gpu_device)  # Get upper-triangle indices\n",
        "        edge_weights = self.mutual_dist_matrix[triu_indices[0], triu_indices[1]]  # Extract edge weights\n",
        "        sorted_indices = torch.argsort(edge_weights)  # Sort edges by weight\n",
        "\n",
        "        parent = torch.arange(n, device=self.cpu_gpu_device)  # Union-Find structure\n",
        "\n",
        "        def find(x):\n",
        "            while parent[x] != x:\n",
        "                x = parent[x]\n",
        "            return x\n",
        "\n",
        "        self.mst_edges = []\n",
        "        for idx in sorted_indices:\n",
        "            u, v = triu_indices[:, idx]\n",
        "            root_u, root_v = find(u), find(v)\n",
        "\n",
        "            if root_u != root_v:  # No cycle condition\n",
        "                self.mst_edges.append((u.item(), v.item(), edge_weights[idx].item()))\n",
        "                parent[root_v] = root_u\n",
        "\n",
        "                if len(self.mst_edges) == n - 1:\n",
        "                    break\n",
        "\n",
        "    def get_mst_edges(self, X):\n",
        "        self.mutual_reachability_distance(X)\n",
        "        self.minimum_spanning_tree()\n",
        "        return self.mst_edges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhuw-C6_5qt8"
      },
      "source": [
        "## ** Clastering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89CGncyq5qt8"
      },
      "outputs": [],
      "source": [
        "class Cluster:\n",
        "    def __init__(\n",
        "        self,\n",
        "        cluster_id,\n",
        "        death_size,\n",
        "        size,\n",
        "        lambda_birth=None,\n",
        "        lambda_death=None,\n",
        "        children=None,\n",
        "        nodes_ids=None,\n",
        "        nodes_lambdas=None,\n",
        "        persistence=0,\n",
        "        is_singleton=False\n",
        "    ):\n",
        "        self.cluster_id = cluster_id\n",
        "\n",
        "        self.death_size = death_size\n",
        "        self.size = size\n",
        "        self.lambda_birth = lambda_birth\n",
        "        self.lambda_death = lambda_death\n",
        "        self.children = children if children is not None else []\n",
        "        self.nodes_ids = nodes_ids if nodes_ids is not None else []\n",
        "        self.nodes_lambdas = nodes_lambdas if nodes_lambdas is not None else []\n",
        "        self.persistence = persistence\n",
        "        self.is_singleton = is_singleton\n",
        "\n",
        "        self.is_noise = True\n",
        "\n",
        "class DisjointSet:\n",
        "    def __init__(self, n_points, min_cluster_size):\n",
        "        self.parents = list(range(n_points))\n",
        "        # For Union-Find optimization (tree height)\n",
        "        self.clusters_ranks = [0] * n_points\n",
        "\n",
        "        self.n_points = n_points\n",
        "        self.min_cluster_size = min_cluster_size or n_points\n",
        "\n",
        "        # Initialize singletons clusters\n",
        "        self.clusters_hierarchy = {}\n",
        "        for i in range(self.n_points):\n",
        "            self.clusters_hierarchy[i] = Cluster(\n",
        "                cluster_id=i,\n",
        "                death_size=0,\n",
        "                size=1,\n",
        "                nodes_ids=[i],\n",
        "                is_singleton=True\n",
        "            )\n",
        "\n",
        "    def find(self, u):\n",
        "        if self.parents[u] != u:\n",
        "            self.parents[u] = self.find(self.parents[u])  # Path compression\n",
        "        return self.parents[u]\n",
        "\n",
        "    def union(self, root_u, root_v):\n",
        "        if self.clusters_ranks[root_u] < self.clusters_ranks[root_v]:\n",
        "            root_u, root_v = root_v, root_u\n",
        "        self.parents[root_v] = root_u\n",
        "        # Rank increases only if equal height\n",
        "        if self.clusters_ranks[root_u] == self.clusters_ranks[root_v]:\n",
        "            self.clusters_ranks[root_u] += 1\n",
        "        return root_u, root_v\n",
        "\n",
        "    def calculate_persistence(self, cluster):\n",
        "        \"\"\"\n",
        "        Calculate persistence for each cluster.\n",
        "        \"\"\"\n",
        "        if cluster.death_size:\n",
        "            delta_lambda = cluster.lambda_death - cluster.lambda_birth\n",
        "            # persistance of existing nodes\n",
        "            cluster.persistence += cluster.death_size * delta_lambda\n",
        "        # persistance of fall off nodes\n",
        "        if len(cluster.nodes_lambdas) > 0:\n",
        "            nodes_lambdas = np.array(cluster.nodes_lambdas)\n",
        "            cluster.persistence += np.sum(nodes_lambdas - cluster.lambda_birth)\n",
        "        cluster.nodes_lambdas = []\n",
        "\n",
        "    # Flatten hierarchy\n",
        "    def collapse_cluster(self, cluster):\n",
        "        self.calculate_persistence(cluster)\n",
        "        if cluster.children == []:\n",
        "            return  # Wait for parent comparison\n",
        "\n",
        "        children_persistence = sum(ch.persistence for ch in cluster.children)\n",
        "        # compare persistences\n",
        "        if cluster.persistence > children_persistence:\n",
        "            cluster.is_noise = False\n",
        "            clusters_to_delete = []\n",
        "            nodes_ids = []\n",
        "\n",
        "            def collect_nodes_and_ids(clust):\n",
        "                nonlocal nodes_ids # Required for extend\n",
        "                for ch in clust.children:\n",
        "                    collect_nodes_and_ids(ch)\n",
        "                    clusters_to_delete.append(ch.cluster_id)\n",
        "                nodes_ids.extend(clust.nodes_ids)\n",
        "\n",
        "            collect_nodes_and_ids(cluster)\n",
        "            cluster.nodes_ids = nodes_ids\n",
        "            cluster.children = []\n",
        "\n",
        "            # Delete after traversal\n",
        "            for cid in clusters_to_delete:\n",
        "                del self.clusters_hierarchy[cid]\n",
        "        else:\n",
        "            cluster.persistence = children_persistence\n",
        "            cluster.is_noise = True\n",
        "\n",
        "    # merge 2 singletons\n",
        "    def merge_the_singletons(self, cluster_u, cluster_v, lambda_val):\n",
        "        cluster_u.death_size = 0\n",
        "        cluster_u.size = 2\n",
        "        cluster_u.nodes_ids.extend(cluster_v.nodes_ids)\n",
        "        cluster_u.nodes_lambdas = [lambda_val, lambda_val]\n",
        "        cluster_u.is_singleton = False\n",
        "        del self.clusters_hierarchy[cluster_v.cluster_id]\n",
        "\n",
        "    # cluster_u absorbs cluster_v\n",
        "    def absorb_cluster(self, cluster_u, cluster_v):\n",
        "        cluster_u.size += cluster_v.size\n",
        "        if len(cluster_u.nodes_ids) >= self.min_cluster_size:\n",
        "            cluster_u.is_noise = False\n",
        "        cluster_u.nodes_ids.extend(cluster_v.nodes_ids)\n",
        "        cluster_u.nodes_lambdas.extend(cluster_v.nodes_lambdas)\n",
        "        del self.clusters_hierarchy[cluster_v.cluster_id]\n",
        "\n",
        "    # cluster_u and cluster_v merged into new cluster\n",
        "    def merge_clusters(self, cluster_u, cluster_v, lambda_val, cluster_id):\n",
        "        cluster_u.lambda_birth = lambda_val\n",
        "        self.collapse_cluster(cluster_u)\n",
        "\n",
        "        cluster_v.lambda_birth = lambda_val\n",
        "        self.collapse_cluster(cluster_v)\n",
        "\n",
        "        size = cluster_u.size + cluster_v.size\n",
        "        children = [cluster_u, cluster_v]\n",
        "\n",
        "        self.clusters_hierarchy[cluster_id] = Cluster(\n",
        "            cluster_id=cluster_id,\n",
        "            death_size=size,\n",
        "            size=size,\n",
        "            lambda_death=lambda_val,\n",
        "            children=children\n",
        "        )\n",
        "\n",
        "    def build_hierarchy(self, mst_edges):\n",
        "        \"\"\"\n",
        "        Build hierarchy from MST\n",
        "        Args:\n",
        "            mst_edges: List of [(u, v, distance), ...] sorted by increasing distance\n",
        "        \"\"\"\n",
        "        epsilon = 1e-10\n",
        "        # Initialize trackers\n",
        "        root_clusters_map = {i: i for i in range(self.n_points)}\n",
        "\n",
        "        # Start assigning new IDs after original points\n",
        "        cluster_id = self.n_points - 1\n",
        "        # Process MST edges in order of increasing distance\n",
        "        for u, v, distance in mst_edges:\n",
        "            # Lambda = inverse of distance\n",
        "            lambda_val = 1 / distance\n",
        "            #lambda_val = 1 / (distance + epsilon)\n",
        "            #lambda_val = -np.log(lambda_val + epsilon)\n",
        "            root_u, root_v = self.find(u), self.find(v)\n",
        "\n",
        "            if root_u != root_v:\n",
        "                # Unite the clusters\n",
        "                root_u, root_v = self.union(root_u, root_v)\n",
        "                # Get the clusters\n",
        "                u_id = root_clusters_map[root_u]\n",
        "                cluster_u = self.clusters_hierarchy[u_id]\n",
        "                v_id = root_clusters_map[root_v]\n",
        "                cluster_v = self.clusters_hierarchy[v_id]\n",
        "\n",
        "                # Absorb or merge\n",
        "                if cluster_u.size < self.min_cluster_size or cluster_v.size < self.min_cluster_size:\n",
        "                    if cluster_u.is_singleton:\n",
        "                        self.merge_the_singletons(\n",
        "                            cluster_u=cluster_u,\n",
        "                            cluster_v=cluster_v,\n",
        "                            lambda_val=lambda_val\n",
        "                        )\n",
        "                    else:\n",
        "                        self.absorb_cluster(\n",
        "                            cluster_u=cluster_u,\n",
        "                            cluster_v=cluster_v\n",
        "                        )\n",
        "                    # Update cluster map\n",
        "                    root_clusters_map[root_u] = cluster_u.cluster_id\n",
        "                else:\n",
        "                    # Increment cluster ID and update cluster map\n",
        "                    cluster_id += 1\n",
        "                    root_clusters_map[root_u] = cluster_id\n",
        "                    # Merge the clusters\n",
        "                    self.merge_clusters(\n",
        "                        cluster_u=cluster_u,\n",
        "                        cluster_v=cluster_v,\n",
        "                        lambda_val=lambda_val,\n",
        "                        cluster_id=cluster_id\n",
        "                    )\n",
        "\n",
        "    def get_clusters_dict(self):\n",
        "        clusters_dict = {\n",
        "            -1: []\n",
        "        }\n",
        "        for cluster_id, cluster in self.clusters_hierarchy.items():\n",
        "            nodes_ids = cluster.nodes_ids\n",
        "            is_noise = cluster.is_noise\n",
        "            if is_noise:\n",
        "                clusters_dict[-1].extend(nodes_ids)\n",
        "            else:\n",
        "                clusters_dict[cluster.cluster_id] = nodes_ids\n",
        "        if len(clusters_dict[-1]) == 0:\n",
        "            del clusters_dict[-1]\n",
        "        return clusters_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a2Uscl65qt9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6MqJXjb5qt9"
      },
      "source": [
        "# * Tests"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "shared_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
