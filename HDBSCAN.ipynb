{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Qyu03gMd5qt4"},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFYZGtm65qt5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1750164346714,"user_tz":300,"elapsed":105,"user":{"displayName":"Dmitri Koltsov","userId":"18251638639959763815"}},"outputId":"d74142f6-df5d-4101-a1fd-a8db42335cd3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["cpu_gpu_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","#cpu_gpu_device = torch.device('cpu')\n","print(f\"Using device: {cpu_gpu_device}\")"]},{"cell_type":"markdown","metadata":{"id":"BKT0vyvO5qt6"},"source":["# * MyHDBScan"]},{"cell_type":"markdown","metadata":{"id":"S_W-OuKB5qt7"},"source":["## ** MST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJyp25065qt7"},"outputs":[],"source":["class MST:\n","    def __init__(\n","        self,\n","        number_of_nearest_neighbors=5,\n","        cpu_gpu_device='cpu',\n","        backend=None,  # Options: None, 'faiss', 'hnsw'\n","        faiss_M=32,\n","        faiss_efConstruction=64,\n","        faiss_efSearch=64,\n","    ):\n","        if number_of_nearest_neighbors < 2:\n","            raise ValueError(\"number_of_nearest_neighbors must be at least 2\")\n","\n","        self.k = number_of_nearest_neighbors\n","\n","        # Validate device\n","        self.cuda_flag = False\n","        if str(cpu_gpu_device).startswith('cuda'):\n","            self.cuda_flag = True\n","            if not torch.cuda.is_available():\n","                raise ValueError(\"CUDA was requested but is not available.\")\n","            try:\n","                torch.cuda.get_device_properties(cpu_gpu_device)\n","            except AssertionError:\n","                raise ValueError(f\"CUDA device '{cpu_gpu_device}' is not available.\")\n","\n","        self.cpu_gpu_device = cpu_gpu_device\n","\n","        if backend not in [None, 'faiss', 'hnsw']:\n","            raise ValueError(\"backend must be one of: None, 'faiss', 'hnsw'\")\n","\n","        self.backend = backend\n","        if self.backend == 'faiss':\n","            import faiss\n","        elif self.backend == 'hnsw':\n","            if self.cuda_flag:\n","                raise ValueError(\"HNSWlib does not support CUDA\")\n","            import hnswlib\n","\n","        self.faiss_M = faiss_M\n","        if self.faiss_M not in [16, 32, 48]:\n","            raise ValueError(\"faiss_M must be one of: 16, 32, 48\")\n","\n","        self.faiss_efConstruction = faiss_efConstruction\n","        if self.faiss_efConstruction not in [32, 64, 128]:\n","            raise ValueError(\"faiss_efConstruction must be one of: 32, 64, 128\")\n","\n","        self.faiss_efSearch = faiss_efSearch\n","        if self.faiss_efSearch not in [32, 64, 128, 256]:\n","            raise ValueError(\"faiss_efSearch must be one of: 32, 64, 128, 256\")\n","\n","    def find(self, x, parent):\n","        while parent[x] != x:\n","            x = parent[x]\n","        return x\n","\n","    def mst_none(self, X, n):\n","        \"\"\"\n","        Compute the mutual reachability distance matrix using PyTorch and CUDA.\n","        \"\"\"\n","        # Compute pairwise Euclidean distances\n","        pairwise_dist = torch.cdist(X, X)\n","\n","        # k-NN distances; +1 accounts for self-distance (zero) in cdist diagonal\n","        knn_distances, _ = torch.topk(pairwise_dist, self.k + 1, largest=False)\n","\n","        # Core distance = distance to k-th nearest neighbor (last column)\n","        # Expand dimensions to broadcast across rows\n","        core_distances = knn_distances[:, -1].unsqueeze(1)\n","\n","        # Compute mutual reachability distance\n","        mutual_dist_matrix = torch.max(pairwise_dist, core_distances)\n","        mutual_dist_matrix = torch.max(mutual_dist_matrix, core_distances.T)\n","\n","        \"\"\"\n","        Compute the Minimum Spanning Tree using Kruskal’s algorithm in PyTorch.\n","        \"\"\"\n","        triu_indices = torch.triu_indices(n, n, 1, device=self.cpu_gpu_device)\n","\n","\n","        # Extract edge weights\n","        edge_weights = mutual_dist_matrix[triu_indices[0], triu_indices[1]]\n","        # Get upper-triangle indices sorted by weights\n","        sorted_indices = torch.argsort(edge_weights)\n","\n","        # Union-Find structure\n","        parent = torch.arange(n, device=self.cpu_gpu_device)\n","\n","        self.mst_edges = []\n","        for idx in sorted_indices:\n","            u, v = triu_indices[:, idx]\n","            root_u, root_v = self.find(u, parent), self.find(v, parent)\n","\n","            if root_u != root_v:  # No cycle condition\n","                self.mst_edges.append((u.item(), v.item(), edge_weights[idx].item()))\n","                parent[root_v] = root_u\n","\n","                if len(self.mst_edges) == n - 1:\n","                    break\n","\n","    def mst_faiss(self, X, n, d):\n","        \"\"\"\n","        Compute the mutual reachability distance matrix using FAISS and CUDA.\n","        \"\"\"\n","        # Create and configure FAISS HNSW index\n","        index = faiss.IndexHNSWFlat(d, self.faiss_M)\n","        index.hnsw.efConstruction = self.faiss_efConstruction\n","        index.hnsw.efSearch = self.faiss_efSearch\n","\n","        # Optional: move to GPU\n","        if self.cuda_flag:\n","            res = faiss.StandardGpuResources()\n","            device = self.cpu_gpu_device.split(':')\n","            device_id = int(device[1]) if len(device) > 1 else 0\n","            index = faiss.index_cpu_to_gpu(res, device_id, index)\n","\n","        # Add data\n","        index.add(X)\n","\n","        # Approximate k-NN search\n","        knn_distances, knn_indices = index.search(X, self.k + 1)\n","        # Core distance = distance to k-th nearest neighbor (last column)\n","        core_distances = knn_distances[:, -1]\n","\n","        if self.cuda_flag:\n","            # Force move to CPU to safely build Python-native edge list\n","            knn_distances = knn_distances.copy()\n","            knn_indices = knn_indices.copy()\n","            core_distances = core_distances.copy()\n","\n","        # Build sparse mutual reachability edge list\n","        edges = []\n","        for i in range(n):\n","            for neighbor_idx, dist in zip(knn_indices[i][1:], knn_distances[i][1:]):  # skip self\n","                j = neighbor_idx\n","                core_i = core_distances[i]\n","                core_j = core_distances[j]\n","                mrd = max(dist, core_i, core_j)\n","                edges.append((i, j, mrd))\n","\n","        \"\"\"\n","        Compute the Minimum Spanning Tree using Kruskal’s algorithm on sparse edges.\n","        \"\"\"\n","        # Union-Find structure\n","        parent = torch.arange(n, device=self.cpu_gpu_device)\n","\n","        # Sort edges by mutual reachability distance (edge weight)\n","        edges = sorted(edges, key=lambda x: x[2])\n","\n","        self.mst_edges = []\n","        for u, v, weight in edges:\n","            root_u, root_v = self.find(u, parent), self.find(v, parent)\n","\n","            if root_u != root_v:\n","                self.mst_edges.append((u, v, weight))\n","                parent[root_v] = root_u\n","\n","                if len(self.mst_edges) == n - 1:\n","                    break\n","\n","    def mst_hnsw(self, X, n, d):\n","        \"\"\"\n","        Compute the mutual reachability distance matrix using HNSWlib and CPU.\n","        \"\"\"\n","        # Initialize HNSWlib index\n","        index = hnswlib.Index(space='l2', dim=d)\n","        index.init_index(max_elements=n, ef_construction=200, M=16)\n","        index.add_items(X)\n","        index.set_ef(50)  # Search effort\n","\n","        # Query k+1 neighbors (including self)\n","        knn_indices, knn_distances = index.knn_query(X, k=self.k + 1)\n","        # Convert squared L2 to actual L2 (Euclidean distances)\n","        knn_distances = np.sqrt(knn_distances)\n","\n","        # Core distance = distance to k-th nearest neighbor (last column)\n","        core_distances = knn_distances[:, -1]\n","\n","        # Build sparse mutual reachability edge list\n","        edges = []\n","        for i in range(n):\n","            for neighbor_idx, dist in zip(knn_indices[i][1:], knn_distances[i][1:]):  # skip self\n","                j = neighbor_idx\n","                core_i = core_distances[i]\n","                core_j = core_distances[j]\n","                mrd = max(dist, core_i, core_j)\n","                edges.append((i, j, mrd))\n","\n","        \"\"\"\n","        Compute the Minimum Spanning Tree using Kruskal’s algorithm on sparse edges.\n","        \"\"\"\n","        # Union-Find structure\n","        parent = torch.arange(n)\n","\n","        # Sort edges by mutual reachability distance (edge weight)\n","        edges = sorted(edges, key=lambda x: x[2])\n","\n","        self.mst_edges = []\n","        for u, v, weight in edges:\n","            root_u, root_v = self.find(u, parent), self.find(v, parent)\n","            if root_u != root_v:\n","                self.mst_edges.append((u, v, weight))\n","                parent[root_v] = root_u\n","                if len(self.mst_edges) == n - 1:\n","                    break\n","\n","    def minimum_spanning_tree(self, X):\n","        # Check input validity: PyTorch tensor or NumPy array 2D\n","        if X.ndim != 2:\n","            raise ValueError(\"Input data X must be 2D: (n_samples, n_features)\")\n","\n","        X_type = None\n","        if isinstance(X, torch.Tensor):\n","            X_type = 'tensor'\n","        if isinstance(X, np.ndarray):\n","            X_type = 'numpy'\n","        if X_type is None:\n","            raise ValueError(\"Input data X must be a PyTorch tensor or NumPy array\")\n","\n","        # Create tree\n","        n, d = X.shape\n","        if self.backend is None:\n","            if X_type == 'tensor':\n","                if X.device != torch.device(self.cpu_gpu_device):\n","                    X = X.to(dtype=torch.float32, device=self.cpu_gpu_device)\n","                else:\n","                    X = X.to(dtype=torch.float32)\n","            else:\n","                X = torch.tensor(X, dtype=torch.float32, device=self.cpu_gpu_device)\n","            self.mst_none(X, n)\n","        else:\n","            if X_type == 'tensor':\n","                X = X.detach().cpu().numpy().astype(np.float32)\n","            if self.backend == 'faiss':\n","                self.mst_faiss(X, n, d)\n","            elif self.backend == 'hnsw':\n","                self.mst_hnsw(X, n, d)\n","\n","    def get_mst_edges(self, X):\n","        self.minimum_spanning_tree(X)\n","        return self.mst_edges"]},{"cell_type":"markdown","metadata":{"id":"jhuw-C6_5qt8"},"source":["## ** Clastering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"89CGncyq5qt8"},"outputs":[],"source":["class Cluster:\n","    def __init__(\n","        self,\n","        cluster_id,\n","        death_size,\n","        size,\n","        lambda_birth=None,\n","        lambda_death=None,\n","        children=None,\n","        nodes_ids=None,\n","        nodes_lambdas=None,\n","        persistence=0,\n","        is_singleton=False\n","    ):\n","        self.cluster_id = cluster_id\n","\n","        self.death_size = death_size\n","        self.size = size\n","        self.lambda_birth = lambda_birth\n","        self.lambda_death = lambda_death\n","        self.children = children if children is not None else []\n","        self.nodes_ids = nodes_ids if nodes_ids is not None else []\n","        self.nodes_lambdas = nodes_lambdas if nodes_lambdas is not None else []\n","        self.persistence = persistence\n","        self.is_singleton = is_singleton\n","\n","        self.is_noise = True\n","\n","class DisjointSet:\n","    def __init__(\n","            self,\n","            n_points,\n","            min_cluster_size,\n","            epsilon=1e-10\n","    ):\n","        self.parents = list(range(n_points))\n","        # For Union-Find optimization (tree height)\n","        self.clusters_ranks = [0] * n_points\n","\n","        self.n_points = n_points\n","        self.min_cluster_size = min_cluster_size\n","        self.epsilon = epsilon\n","\n","        # Initialize singletons clusters\n","        self.clusters_hierarchy = {}\n","        for i in range(self.n_points):\n","            self.clusters_hierarchy[i] = Cluster(\n","                cluster_id=i,\n","                death_size=0,\n","                size=1,\n","                nodes_ids=[i],\n","                is_singleton=True\n","            )\n","\n","        #self.debugging_history = []\n","\n","    def find(self, u):\n","        if self.parents[u] != u:\n","            self.parents[u] = self.find(self.parents[u])  # Path compression\n","        return self.parents[u]\n","\n","    def union(self, root_u, root_v):\n","        if self.clusters_ranks[root_u] < self.clusters_ranks[root_v]:\n","            root_u, root_v = root_v, root_u\n","        self.parents[root_v] = root_u\n","        # Rank increases only if equal height\n","        if self.clusters_ranks[root_u] == self.clusters_ranks[root_v]:\n","            self.clusters_ranks[root_u] += 1\n","        return root_u, root_v\n","\n","    def calculate_persistence(self, cluster):\n","        \"\"\"\n","        Calculate persistence for each cluster.\n","        \"\"\"\n","        if cluster.death_size:\n","            delta_lambda = cluster.lambda_death - cluster.lambda_birth\n","            # persistance of existing nodes\n","            cluster.persistence += cluster.death_size * delta_lambda\n","        # persistance of fall off nodes\n","        if len(cluster.nodes_lambdas) > 0:\n","            nodes_lambdas = np.array(cluster.nodes_lambdas)\n","            cluster.persistence += np.sum(nodes_lambdas - cluster.lambda_birth)\n","        cluster.nodes_lambdas = []\n","\n","    # Flatten hierarchy\n","    def collapse_cluster(self, cluster):\n","        self.calculate_persistence(cluster)\n","        if cluster.children == []:\n","            return  # Wait for parent comparison\n","\n","        children_persistence = sum(ch.persistence for ch in cluster.children)\n","        # compare persistences\n","        if cluster.persistence > children_persistence:\n","            cluster.is_noise = False\n","            clusters_to_delete = []\n","            nodes_ids = []\n","\n","            def collect_nodes_and_ids(clust):\n","                nonlocal nodes_ids # Required for extend\n","                for ch in clust.children:\n","                    collect_nodes_and_ids(ch)\n","                    clusters_to_delete.append(ch.cluster_id)\n","                nodes_ids.extend(clust.nodes_ids)\n","\n","            collect_nodes_and_ids(cluster)\n","            cluster.nodes_ids = nodes_ids\n","            cluster.children = []\n","\n","            # Delete after traversal\n","            for cid in clusters_to_delete:\n","                del self.clusters_hierarchy[cid]\n","        else:\n","            cluster.persistence = children_persistence\n","            cluster.is_noise = True\n","\n","    # merge 2 singletons\n","    def merge_the_singletons(self, cluster_u, cluster_v, lambda_val):\n","        cluster_u.death_size = 0\n","        cluster_u.size = 2\n","        cluster_u.nodes_ids.extend(cluster_v.nodes_ids)\n","        cluster_u.nodes_lambdas = [lambda_val, lambda_val]\n","        cluster_u.is_singleton = False\n","        del self.clusters_hierarchy[cluster_v.cluster_id]\n","\n","    # cluster_u absorbs cluster_v\n","    def absorb_cluster(self, cluster_u, cluster_v):\n","        cluster_u.size += cluster_v.size\n","        if len(cluster_u.nodes_ids) >= self.min_cluster_size:\n","            cluster_u.is_noise = False\n","        cluster_u.nodes_ids.extend(cluster_v.nodes_ids)\n","        cluster_u.nodes_lambdas.extend(cluster_v.nodes_lambdas)\n","        del self.clusters_hierarchy[cluster_v.cluster_id]\n","\n","    # cluster_u and cluster_v merged into new cluster\n","    def merge_clusters(self, cluster_u, cluster_v, lambda_val, cluster_id):\n","        cluster_u.lambda_birth = lambda_val\n","        self.collapse_cluster(cluster_u)\n","\n","        cluster_v.lambda_birth = lambda_val\n","        self.collapse_cluster(cluster_v)\n","\n","        size = cluster_u.size + cluster_v.size\n","        children = [cluster_u, cluster_v]\n","\n","        self.clusters_hierarchy[cluster_id] = Cluster(\n","            cluster_id=cluster_id,\n","            death_size=size,\n","            size=size,\n","            lambda_death=lambda_val,\n","            children=children\n","        )\n","\n","    def build_hierarchy(self, mst_edges):\n","        \"\"\"\n","        Build hierarchy from MST\n","        Args:\n","            mst_edges: List of [(u, v, distance), ...] sorted by increasing distance\n","        \"\"\"\n","        # Initialize trackers\n","        root_clusters_map = {i: i for i in range(self.n_points)}\n","\n","        # Start assigning new IDs after original points\n","        cluster_id = self.n_points - 1\n","        # Process MST edges in order of increasing distance\n","        for u, v, distance in mst_edges:\n","            # Lambda = inverse of distance\n","            #lambda_val = 1 / distance\n","            lambda_val = 1 / (distance + self.epsilon)\n","            #lambda_val = -np.log(lambda_val + self.epsilon)\n","            root_u, root_v = self.find(u), self.find(v)\n","\n","            # debug_dict = {\n","            #     'u': u,\n","            #     'v': v\n","            # }\n","\n","            if root_u != root_v:\n","                # Unite the clusters\n","                root_u, root_v = self.union(root_u, root_v)\n","                # Get the clusters\n","                u_id = root_clusters_map[root_u]\n","                cluster_u = self.clusters_hierarchy[u_id]\n","                v_id = root_clusters_map[root_v]\n","                cluster_v = self.clusters_hierarchy[v_id]\n","                # debug_dict['u_id'] = u_id\n","                # debug_dict['v_id'] = v_id\n","                # debug_dict['u_nodes'] = copy.deepcopy(cluster_u.nodes_ids)\n","                # debug_dict['v_nodes'] = copy.deepcopy(cluster_v.nodes_ids)\n","                # Absorb or merge\n","                if cluster_u.size < self.min_cluster_size or cluster_v.size < self.min_cluster_size:\n","                    if cluster_u.is_singleton:\n","                        # debug_dict['action'] = 'merge_the_singletons'\n","\n","                        self.merge_the_singletons(\n","                            cluster_u=cluster_u,\n","                            cluster_v=cluster_v,\n","                            lambda_val=lambda_val\n","                        )\n","                        new_cluster_id = cluster_u.cluster_id\n","\n","                    else:\n","                        # debug_dict['action'] = 'absorb'\n","\n","                        self.absorb_cluster(\n","                            cluster_u=cluster_u,\n","                            cluster_v=cluster_v\n","                        )\n","\n","                        new_cluster_id = cluster_u.cluster_id\n","\n","                    # Update cluster map\n","                    root_clusters_map[root_u] = cluster_u.cluster_id\n","                else:\n","                    # Increment cluster ID and update cluster map\n","                    cluster_id += 1\n","                    root_clusters_map[root_u] = cluster_id\n","                    # Merge the clusters\n","\n","                    # debug_dict['action'] = 'merge'\n","\n","                    self.merge_clusters(\n","                        cluster_u=cluster_u,\n","                        cluster_v=cluster_v,\n","                        lambda_val=lambda_val,\n","                        cluster_id=cluster_id\n","                    )\n","                    new_cluster_id = cluster_id\n","\n","            new_cluster = self.clusters_hierarchy[new_cluster_id]\n","\n","            # debug_dict['nodes_ids'] = copy.deepcopy(new_cluster.nodes_ids)\n","            # debug_dict['size'] = new_cluster.size\n","            # debug_dict['cluster_id'] = new_cluster_id\n","            # debug_dict['is_noise'] = new_cluster.is_noise\n","\n","            # debug_dict['final clusters'] = copy.deepcopy(self.final_clusters)\n","            # self.debugging_history.append(debug_dict)\n","\n","        # root_cluster = self.clusters_hierarchy[root_id]\n","        # root_cluster.lambda_birth = 0\n","        # self.collapse_cluster(root_cluster)\n","\n","    def get_clusters_dict(self):\n","        clusters_dict = {\n","            -1: []\n","        }\n","        for cluster_id, cluster in self.clusters_hierarchy.items():\n","            nodes_ids = cluster.nodes_ids\n","            is_noise = cluster.is_noise\n","            if is_noise:\n","                clusters_dict[-1].extend(nodes_ids)\n","            else:\n","                clusters_dict[cluster.cluster_id] = nodes_ids\n","        if len(clusters_dict[-1]) == 0:\n","            del clusters_dict[-1]\n","        return clusters_dict\n","\n","    def get_labels(self):\n","        labels = np.full(self.n_points, -1)\n","        for cluster_id, cluster in self.clusters_hierarchy.items():\n","            if cluster.is_noise:\n","                labels[cluster.nodes_ids] = -1\n","            else:\n","                labels[cluster.nodes_ids] = cluster_id\n","        return labels\n"]},{"cell_type":"markdown","source":["## ** main class"],"metadata":{"id":"lhjbvk-LwC2U"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4a2Uscl65qt9"},"outputs":[],"source":["class HDBSCAN:\n","    def __init__(\n","            self,\n","            min_cluster_size=5,\n","            number_of_nearest_neighbors=None,\n","            cpu_gpu_device='cpu',\n","            backend=None,  # Options: None, 'faiss', 'hnsw'\n","            faiss_M=32,\n","            faiss_efConstruction=64,\n","            faiss_efSearch=64,\n","            epsilon=1e-10\n","    ):\n","        number_of_nearest_neighbors = number_of_nearest_neighbors or min_cluster_size\n","        if number_of_nearest_neighbors > min_cluster_size:\n","            raise ValueError(\"number_of_nearest_neighbors should be <= min_cluster_size\")\n","        self.mst_agent = MST(\n","            number_of_nearest_neighbors=number_of_nearest_neighbors,\n","            cpu_gpu_device=cpu_gpu_device,\n","            backend=backend,\n","            faiss_M=faiss_M,\n","            faiss_efConstruction=faiss_efConstruction,\n","            faiss_efSearch=faiss_efSearch\n","        )\n","\n","        self.min_cluster_size = min_cluster_size\n","        self.epsilon = epsilon\n","\n","    def fit(self, X):\n","        n_points = X.shape[0]\n","        if n_points <= self.min_cluster_size:\n","            raise ValueError(\"Amount of data points should be > min_cluster_size\")\n","\n","        mst_edges = self.mst_agent.get_mst_edges(X)\n","\n","        ds_agent = DisjointSet(\n","            n_points=n_points,\n","            min_cluster_size=self.min_cluster_size,\n","            epsilon=self.epsilon\n","        )\n","\n","        ds_agent.build_hierarchy(mst_edges)\n","        self.labels_ = ds_agent.get_labels()\n","\n","        return self\n","\n","    def fit_predict(self, X):\n","        self.fit(X)\n","        return self.labels_"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"colab":{"provenance":[{"file_id":"1LktkBXlYqvOg2Oo0h26T6i7PHXRmLabz","timestamp":1750170346159},{"file_id":"16xlHtAp0PgKliUU_yu-98BlfK0ALNBru","timestamp":1750086118668},{"file_id":"https://github.com/Dim314159/unsupervised/blob/main/HDBSCAN.ipynb","timestamp":1743337661479}],"collapsed_sections":["S_W-OuKB5qt7","jhuw-C6_5qt8"],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}